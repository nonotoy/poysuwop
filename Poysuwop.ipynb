{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nonotoy/poysuwop/blob/main/Poysuwop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5gwQYRaSqgW"
      },
      "source": [
        "# Poysuwop / Translation library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYMLa890u4zv"
      },
      "source": [
        "## 0: Library install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STeQfaU9TdSd"
      },
      "outputs": [],
      "source": [
        "# Library\n",
        "import glob\n",
        "import json\n",
        "import re\n",
        "import collections\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.cuda.amp import GradScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    MBartForConditionalGeneration,\n",
        "    MBart50Tokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    EarlyStoppingCallback,\n",
        "    RobertaTokenizerFast,\n",
        "    GenerationConfig,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "from datasets import DatasetDict, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sacrebleu\n",
        "import MeCab\n",
        "import gc\n",
        "\n",
        "from googletrans import Translator\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnUKNDL0TnsS"
      },
      "source": [
        "## 1: Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G6Juda8ykdG"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df, src_lang, tgt_lang, tokenizer):\n",
        "        self.df = df\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source_text = self.df.iloc[idx][self.src_lang]\n",
        "        target_text = self.df.iloc[idx][self.tgt_lang]\n",
        "\n",
        "        if not isinstance(source_text, str):\n",
        "            source_text = str(source_text)\n",
        "\n",
        "        if not isinstance(target_text, str):\n",
        "            target_text = str(target_text)\n",
        "\n",
        "        source = self.tokenizer(source_text, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        target = self.tokenizer(target_text, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        source[\"labels\"] = target[\"input_ids\"]\n",
        "\n",
        "        return source\n",
        "\n",
        "\n",
        "def create_finetuned_model(model, train_df, test_df, src_lang, tgt_lang, tokenizer, save_path):\n",
        "\n",
        "    # assign GPU to the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Convert df to string\n",
        "    train_df[src_lang] = train_df[src_lang].astype(str)\n",
        "    train_df[tgt_lang] = train_df[tgt_lang].astype(str)\n",
        "    test_df[src_lang] = test_df[src_lang].astype(str)\n",
        "    test_df[tgt_lang] = test_df[tgt_lang].astype(str)\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset = TranslationDataset(train_df, src_lang, tgt_lang, tokenizer)\n",
        "    eval_dataset = TranslationDataset(test_df, src_lang, tgt_lang, tokenizer)\n",
        "\n",
        "    # DataLoader\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=4, shuffle=False, pin_memory=True)\n",
        "\n",
        "    # Params\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    total_steps = len(train_dataloader) * 30\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    model.train()\n",
        "    previous_eval_loss = float('inf')\n",
        "\n",
        "    for epoch in range(30):\n",
        "        for batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in eval_dataloader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**batch)\n",
        "                eval_loss += outputs.loss.item()\n",
        "\n",
        "        eval_loss /= len(eval_dataloader)\n",
        "        print(f\"Epoch {epoch+1} completed: Validation Loss: {eval_loss}\")\n",
        "        model.train()\n",
        "\n",
        "        # Early stopping logic\n",
        "        if epoch > 2 and eval_loss >= previous_eval_loss:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "        previous_eval_loss = eval_loss\n",
        "\n",
        "    # Save model & tokenizer\n",
        "    model.save_pretrained(save_path)\n",
        "\n",
        "    if src_lang == 'ain':\n",
        "        tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    # Release memory and cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize(texts, tokenizer):\n",
        "    inputs = texts['source']\n",
        "    targets = texts['target']\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxVwso_2P4HM"
      },
      "source": [
        "## 2: Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEY5EyDYKo9v"
      },
      "outputs": [],
      "source": [
        "def translate(text, src_lang, forward_model, forward_tokenizer, tgt_lang, backward_model, backward_tokenizer):\n",
        "\n",
        "    if src_lang == 'ain':\n",
        "        model = forward_model\n",
        "        tokenizer = forward_tokenizer\n",
        "\n",
        "    elif src_lang == 'jpn':\n",
        "        model = backward_model\n",
        "        tokenizer = backward_tokenizer\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"src_lang must be 'ain' or 'jpn'.\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
        "\n",
        "    # Translate\n",
        "    translated_tokens = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True, decoder_start_token_id=model.config.decoder_start_token_id)\n",
        "\n",
        "    # Decode\n",
        "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "def translate_batch(texts, model, tokenizer):\n",
        "\n",
        "    if not isinstance(texts, list):\n",
        "        raise TypeError(\"texts must be a list of strings\")\n",
        "    for text in texts:\n",
        "        if not isinstance(text, str):\n",
        "            raise TypeError(\"Each element in texts must be a string\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Enable Gradient Checkpointing: 純伝播時の計算結果は消去して、逆伝播時に勾配を再計算するようにする\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=1024,\n",
        "        add_special_tokens=False\n",
        "    ).to(device)\n",
        "\n",
        "    # Translate\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        translated_tokens = model.generate(\n",
        "            **inputs,\n",
        "            max_length=1024,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            decoder_start_token_id=model.config.decoder_start_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    translated_texts = tokenizer.batch_decode(\n",
        "        translated_tokens,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "        errors='ignore')\n",
        "\n",
        "    return translated_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXJ0UB4LNR9E"
      },
      "source": [
        "## 3: Cyclic translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmzzX1HTKPvO"
      },
      "outputs": [],
      "source": [
        "# ain-jpn-ain\n",
        "def cyclic_translate(df, src_lang, forward_model, forward_tokenizer, backward_model, backward_tokenizer, batch_size=32):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    forward_model.to(device)\n",
        "    backward_model.to(device)\n",
        "\n",
        "    # Set lists from corpus dataframe\n",
        "    texts_no = df['no.'].tolist()\n",
        "    src_texts = df[src_lang].tolist()\n",
        "    src_texts = [str(text) for text in src_texts]\n",
        "\n",
        "    translated_texts = []\n",
        "    backtranslated_texts = []\n",
        "\n",
        "    batch_counter = 0\n",
        "\n",
        "    for i in range(0, len(src_texts), batch_size):\n",
        "\n",
        "        if batch_counter % (1000 // batch_size) == 0 or i + batch_size >= len(src_texts):\n",
        "            print(f\"Processing batch {i // batch_size} / {len(src_texts)}\")\n",
        "\n",
        "        batch_counter += 1\n",
        "\n",
        "        batch_texts = src_texts[i:i+batch_size]\n",
        "\n",
        "        # Translate from ain to jpn\n",
        "        translated_batch = translate_batch(batch_texts, forward_model, forward_tokenizer)\n",
        "\n",
        "        # Back translate from jpn to ain\n",
        "        backtranslated_batch = translate_batch(translated_batch, backward_model, backward_tokenizer)\n",
        "\n",
        "        translated_texts.extend(translated_batch)\n",
        "        backtranslated_texts.extend(backtranslated_batch)\n",
        "\n",
        "    # add to df\n",
        "    df['translated_jpn'] = translated_texts\n",
        "    df['backtranslated_ain'] = backtranslated_texts\n",
        "\n",
        "    # Release memory and cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOXVDGgyegId"
      },
      "outputs": [],
      "source": [
        "# Retry up to 3 times with exponential backoff\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def translate_with_retry(text, src, dest):\n",
        "    translator = Translator()\n",
        "    return translator.translate(text, src=src, dest=dest)\n",
        "\n",
        "# jpn-eng-jpn\n",
        "def cyclic_google_translate(df):\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        original = row['jpn']\n",
        "\n",
        "        translated = translate_with_retry(original, src='ja', dest='en')\n",
        "        back_translated = translate_with_retry(translated.text, src='en', dest='ja')\n",
        "\n",
        "        df.loc[i, 'translated_eng'] = translated.text\n",
        "        df.loc[i, 'backtranslated_jpn'] = back_translated.text\n",
        "\n",
        "        if i % 5000 == 0 or i == len(df):\n",
        "            print(i, '/', len(df))\n",
        "\n",
        "    # Release memory and cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print('done')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qk7gp9bURE"
      },
      "source": [
        "## 4: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPCHrywHSm-p"
      },
      "outputs": [],
      "source": [
        "def evaluate(df):\n",
        "    references = df['ain'].astype(str).tolist()\n",
        "    translations = df['backtranslated_ain'].astype(str).tolist()\n",
        "\n",
        "    # BLEU\n",
        "    bleu = sacrebleu.corpus_bleu(translations, [references])\n",
        "\n",
        "    # chrF\n",
        "    chrf = sacrebleu.corpus_chrf(translations, [references])\n",
        "\n",
        "    # TER\n",
        "    ter = sacrebleu.corpus_ter(translations, [references])\n",
        "\n",
        "    score = {\n",
        "        'BLEU': bleu.score,\n",
        "        'chrF': chrf.score,\n",
        "        'TER': ter.score\n",
        "    }\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrodKiPAM5nb"
      },
      "source": [
        "## 5: Merge backtranslated text to original df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VF_0PKcNDek"
      },
      "outputs": [],
      "source": [
        "def merge_backtranslated_text(df, original_df, cycle, dataset):\n",
        "\n",
        "    if dataset == 'ain':\n",
        "\n",
        "        if 'backtranslated_ain' not in df.columns or 'translated_jpn' not in df.columns:\n",
        "            return df\n",
        "\n",
        "        df_tmp = df[[\n",
        "            'no.',\n",
        "            #'ain', # Original sentence\n",
        "            'translated_jpn',\n",
        "            'backtranslated_ain'\n",
        "        ]].copy()\n",
        "\n",
        "        # change column name\n",
        "        #df_tmp.columns = ['no.', 'ain', 'jpn', 'ain-bktr']\n",
        "        df_tmp.columns = ['no.', 'jpn', 'ain']\n",
        "\n",
        "        # add column backtranslated count to temp df with value '0'\n",
        "        df_tmp['src_backtranslated_cycles'] = str(cycle)\n",
        "\n",
        "        # add column 'backtranslated' to the original df with value '0'\n",
        "        original_df['src_backtranslated_cycles'] = '0'\n",
        "\n",
        "    elif dataset == 'jpn':\n",
        "\n",
        "        if 'backtranslated_jpn' not in df.columns or 'translated_eng' not in df.columns:\n",
        "            return df\n",
        "\n",
        "        df_tmp = df[[\n",
        "            'no.',\n",
        "            'jpn', # Original sentence\n",
        "            'translated_eng',\n",
        "            'backtranslated_jpn'\n",
        "        ]].copy()\n",
        "\n",
        "        # change column name\n",
        "        df_tmp.columns = ['no.', 'jpn', 'eng', 'jpn-bktr']\n",
        "\n",
        "        # add column backtranslated count to temp df with value '0'\n",
        "        df_tmp['tgt_backtranslated_cycles'] = str(cycle)\n",
        "\n",
        "        # add column 'backtranslated' to the original df with value '0'\n",
        "        original_df['tgt_backtranslated_cycles'] = '0'\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"dataset must be 'ain' or 'jpn'.\")\n",
        "\n",
        "    # sentence number reformat: padding with 0 '########'\n",
        "    df_tmp['no.'] = df_tmp['no.'].astype(str).apply(lambda x: x.zfill(8))\n",
        "\n",
        "    # Add to the original df\n",
        "    df = pd.concat([original_df, df_tmp], ignore_index=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6: Create Path"
      ],
      "metadata": {
        "id": "9R8Aaj4TMiDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_path(csv_prefix, direction, cycle):\n",
        "\n",
        "    src_lang = 'ain'\n",
        "    tgt_lang = 'jpn'\n",
        "\n",
        "    # Models\n",
        "    forward_model_path = f'models/{csv_prefix}/{direction}_{src_lang}-{tgt_lang}_c{cycle}'\n",
        "    backward_model_path = f'models/{csv_prefix}/{direction}_{tgt_lang}-{src_lang}_c{cycle}'\n",
        "\n",
        "    # csv Files\n",
        "    train_file = f'temp/{csv_prefix}/{direction}_c{cycle}_train.txt'\n",
        "    test_file = f'temp/{csv_prefix}/{direction}_c{cycle}_test.txt'\n",
        "    result_file = f'translation_result/{csv_prefix}/{direction}_c{cycle}.txt'\n",
        "\n",
        "    return forward_model_path, backward_model_path, train_file, test_file, result_file"
      ],
      "metadata": {
        "id": "3dGo4nCJMhh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "19nYHHpjoZHu5iumttqJGfCCs_CM3EZCP",
      "authorship_tag": "ABX9TyOvMNuHiZ15Uu5jGPpR5QIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}