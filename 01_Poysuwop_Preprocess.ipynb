{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1jnsdgDLwEVI1nWH4MAWNGGfAK3HLYayQ",
      "authorship_tag": "ABX9TyO2zJ/nHBydf7LmHX+il3nb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Poysuwop_Nr1 / Preprocess\n",
        "Preprocess Ainu texts and create the bilingual (ain/jpn) corpus"
      ],
      "metadata": {
        "id": "Z7SKsQa75JJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Library install"
      ],
      "metadata": {
        "id": "Aks1fkAFQH_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Library\n",
        "import glob\n",
        "import json\n",
        "import re\n",
        "import collections\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Change Current Directory\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/Poysuwop')\n",
        "\n",
        "# Load preprocess module\n",
        "#sys.path.append('/content/drive/MyDrive/Colab Notebooks/Poysuwop')\n",
        "from modules import ainPreprocess"
      ],
      "metadata": {
        "id": "RqFRTi9qQQYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test preprocess module"
      ],
      "metadata": {
        "id": "I4ZFmVaCRgtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -- ex. --\n",
        "sentence = 'an=an'\n",
        "print(sentence)\n",
        "print(ainPreprocess.preprocess(sentence))\n",
        "\n",
        "# -- ex. --\n",
        "sentence = 'ohonno somo unukar=an'\n",
        "print(sentence)\n",
        "print(ainPreprocess.preprocess(sentence))\n",
        "\n",
        "# -- ex. --\n",
        "# sentence = 'ク・ネㇷ゚キ　ヒ　カ　エン・ココパンパ　コㇿカ　ネ　ワ　アンペ　カ　ケシㇼキラㇷ゚　ワ　ク・ネㇷ゚キ　ルスイ　ワ　ケシㇼキラㇷ゚　ワ　クス　（ク・ネㇷ゚キ）　チセ　ソイ　ペカ　クネㇷ゚キ　コㇿ　カン。'\n",
        "# sentence = 'ペウレクﾙ　ネ　コﾛカ　ア・アイヌコﾛ'\n",
        "sentence = '“Shirokanipe ranran pishkan, konkanipe ranran pishkan.” arian rekpo chiki kane petesoro sapash aine, ainukotan enkashike chikush kor shichorpokun inkarash ko teeta wenkur tane nishpa ne, teeta nishpa tane wenkur ne kotom shiran.'\n",
        "print(sentence)\n",
        "print(ainPreprocess.preprocess(sentence))\n",
        "\n",
        "# -- ex. --\n",
        "sentence = 'vv yayán ainu ku=ne ruwe ne korka, ainu itak ani Transformer[123] ku=kor_ rushuy un!'\n",
        "print(sentence)\n",
        "print(ainPreprocess.preprocess(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDFPmVO4ReFr",
        "outputId": "19f465f4-f8b8-404e-b0f4-912542508f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "an=an\n",
            "an þan\n",
            "ohonno somo unukar=an\n",
            "ohonno somo unukar þan\n",
            "“Shirokanipe ranran pishkan, konkanipe ranran pishkan.” arian rekpo chiki kane petesoro sapash aine, ainukotan enkashike chikush kor shichorpokun inkarash ko teeta wenkur tane nishpa ne, teeta nishpa tane wenkur ne kotom shiran.\n",
            "sirokanipe ranran piskan konkanipe ranran piskan arian rekpo ciki kane petesoro sapas aine ainukotan enkasike cikus kor sicorpokun inkaras ko teeta wenkur tane nispa ne teeta nispa tane wenkur ne kotom siran\n",
            "vv yayán ainu ku=ne ruwe ne korka, ainu itak ani Transformer[123] ku=kor_ rushuy un!\n",
            "yayan ainu kuþ ne ruwe ne korka ainu itak ani transformer kuþ kor rusuy un\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "VO2XTG7-MvUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch sentence using the 'ain' tag from .json file\n",
        "# ['ain']       romanized Ainu sentence\n",
        "# ['ain-kana']  Ainu sentence transcribed in Ainu Kana\n",
        "# ['jpn']       Japanese sentence\n",
        "\n",
        "def alphabetCheck(text):\n",
        "    for char in text:\n",
        "        if not (char.isascii() and (char.isalpha() or char.isspace() or 'þ' or \"'\")):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "corpus_texts = []\n",
        "ain_raw_texts = []\n",
        "ain_preprocessed_texts = []\n",
        "jpn_texts = []\n",
        "verbalslip_pattern = r'(^| [a-zA-Z]*)(\\.|x|X){2,}'\n",
        "\n",
        "for f in glob.glob('/content/drive/MyDrive/Colab Notebooks/Poysuwop/corpus_json/*.json'):\n",
        "\n",
        "    # load .json\n",
        "    data = json.load(open(str(f)))\n",
        "\n",
        "    for key, value in data.items():\n",
        "\n",
        "        if key != 'code' and key != 'title':\n",
        "\n",
        "            ain_sentence = value['ain'] if 'ain' in value else ''\n",
        "            jpn_sentence = value['jpn'] if 'jpn' in value else ''\n",
        "\n",
        "            if len(ain_sentence) > 0 and len(jpn_sentence) > 0 and alphabetCheck(ain_sentence) == True:\n",
        "\n",
        "                # Verbal slip: Words with slips should be removed, since it may produce non-existing words\n",
        "                # This procedure should be omitted from input to Transformer model in order not to create a non-sentence\n",
        "                if re.search(verbalslip_pattern, ain_sentence):\n",
        "                    ain_sentence = re.sub(verbalslip_pattern, ' ', ain_sentence)\n",
        "                    ain_sentence = ' '.join(ain_sentence.split())\n",
        "\n",
        "                new_ain_sentence = ainPreprocess.preprocess(ain_sentence)\n",
        "\n",
        "                corpus_texts.append(new_ain_sentence + '\\t' + jpn_sentence)\n",
        "                ain_preprocessed_texts.append(new_ain_sentence)\n",
        "                jpn_texts.append(jpn_sentence)\n",
        "                ain_raw_texts.append(ain_sentence)"
      ],
      "metadata": {
        "id": "ICFPDROIYge7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_TBRcharacters(texts):\n",
        "\n",
        "    ##### Check charcters used in the dataset\n",
        "    charlist = [char for text in texts for char in list(text)]\n",
        "    charlist = [k for k, v in collections.Counter(charlist).items() if v > 1]\n",
        "    charlist_TBR = ''.join(str(x) for x in charlist)\n",
        "    char_TBR = re.sub(r\"[a-zÞA-Z0-9 =]\",\"\",charlist_TBR)\n",
        "    print(\"Charcaters to be removed: {0}\".format(char_TBR))\n",
        "\n",
        "\n",
        "# check characters to be removed\n",
        "check_TBRcharacters(ain_raw_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s04q0pehMbe-",
        "outputId": "6cfbabeb-14ca-49e6-a324-3377749e3fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charcaters to be removed: .,'?()_!/\"-:;[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('poysuwop_corpus.txt', 'w', encoding='utf-8') as file:\n",
        "    for i in range(len(corpus_texts)):\n",
        "        file.write(corpus_texts[i] + '\\n')\n",
        "\n",
        "with open('poysuwop_ain.txt', 'w', encoding='utf-8') as file:\n",
        "    for i in range(len(ain_preprocessed_texts)):\n",
        "        file.write(ain_preprocessed_texts[i] + '\\n')\n",
        "\n",
        "with open('poysuwop_ain_raw.txt', 'w', encoding='utf-8') as file:\n",
        "    for i in range(len(ain_raw_texts)):\n",
        "        file.write(ain_raw_texts[i] + '\\n')\n",
        "\n",
        "with open('poysuwop_jpn.txt', 'w', encoding='utf-8') as file:\n",
        "    for i in range(len(jpn_texts)):\n",
        "        file.write(jpn_texts[i] + '\\n')"
      ],
      "metadata": {
        "id": "0kyt9vyfXwO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check"
      ],
      "metadata": {
        "id": "1DR-i5JST6wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- ex. --\n",
        "print(corpus_texts[1])\n",
        "#print(preprocessed_texts[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9Wl8sFCShb0",
        "outputId": "da3fb64b-0525-4e15-8705-edfce54a0386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kþ erampewtek\t私はわからない\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Affix Marker Check"
      ],
      "metadata": {
        "id": "fcnvjNsIXafo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 人称接辞マーカー = の両端に文字が残っているものの抽出\n",
        "for i in range(len(texts)):\n",
        "    if re.search(r'\\S+=\\S+', texts[i]):\n",
        "        print(texts[i])"
      ],
      "metadata": {
        "id": "8Y0IqJvxbFxz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "8e37ad62-6538-445b-9ce6-7d76965da178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'texts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3f612a5b656e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 人称接辞マーカー = の両端に文字が残っているものの抽出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\S+=\\S+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(texts)):\n",
        "    if re.search(r'[lqvx]', texts[i]):\n",
        "        print(texts[i])"
      ],
      "metadata": {
        "id": "r97QF84hLzHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}